This is the initial input document, and main basis for this all.

# Laegna Machine Learning Basics

This is the introduction to LaegnaAI2MLBasics.

Let's apply a logarithm to ChatGPT models. We arrive linear processing: this is the mister AI, while LaegnaAIBasics => LaegnaAIDLBasics, which introduces a Perceptron: an AI, today, is known mostly by the lady in this DL *revolution*.

- A lady, for granted, seeks a man.

## Optimization

Is an alchemy between a female AI - exponential growth, emotional, infinity-oriented and "flow" consciousness; where a man is logarithmic growth into a deep metscience: from *one octave up*, it's the linear growth, where a woman seeks linear growth in exponential space.

Optimization 2/2, half to half (I use *discrete* numbers, linearizing below zero on complex axes, so it's $2/2$, a magic formulae):

- Consider you have 10000 units of memory space (in my numbers, it's 9999, because 0 is converted to U and binary base seem rather like from 9 to 1, 2, 3, ..., 8; I need to count *volume* of number units passing, and this is the simple complex).

## Complexity draft

Machine gives a complexity draft:

- We use complex numbers

- We use x, $x_{1}$ , a depth layer with one set of bias and weight from y, $x_{2}$.

- Input complex number uses two bias and weight tables, but in Perceptron terms we look at 1\*1-tensor and 1\*0-tensor, given dimension size and dimension count (a dimensionality). We use 1*1 element 2D matrix and 1 element 1D list.

##### Female robot

The funky female robot who has flow of code and expressions with you, and masters your tools unlike any other automata-based or small robot did before:

- Uses exponential space.

- You can apply exponential factors to this calculation; to apply Perceptron model to back up ML, you can create a square dimension connections between elements; you use square matrix for all input and output variables, having inputs in first and outputs in second window; they can scroll it merged or aligned to an AI. But for machine learning, you normalize it to form of identity matrix or only dimensional averages, where 0's dismiss the number and 1 has power 1 of the operation; you use frequency (octave in my terms) calculation, where linear display is calculated ibn volume, and in sinus or cosinus wave: rather binary impulses of specific signals of elements; complex number turns this into phased, frequency based dimension closer to full psychics framework for neural activity.

- Half of your memory: take square root of this, and you can do vetor\*vector and element\*element matrix calculus: it does this in exponential space with a female power of intuition and flow.



Space, how each cell is connected to each cell in nerval activation layers:



I call this *Etom*:

* Wrapper of Atoms into molecules

* Each layer has atomic connection from Atom to Atom

* They operate with same locality in time, as locality of multiconnected cells in space - the matrix:
  
  * Directly connecting first => second => first element in 1 to 1 connection
  
  * Etom Class: *layers of an AI*; it's our in-between variable we won't need after declaring what is molecule, the object we need in complex space.
  
  * Has imaginary size in little-endian (real size otherwise): let's use imaginary part of complex number we mostly need to swap by dimension for any non-linear ordering.

* Dimensional window
  
  * We count dimensions below and above.
  
  * We use half of memory for steps of this AI:
    
    * Single layer or attention layer calculation
    
    * Processes elementwise connections and turns this into higher-space, more abstract, projection.
    
    * Resolves flows: tries to get a flow in complex world.
    
    * Each single step optimization: half time (processing) half space (memory), taken on GPU and it's memory.
  
  * Step size for single processing measured in Etoms - epochs in training and fine-tuning, or layers otherwise.



**Male robot**

Inside tools, follows algorithms in rational languages like cobol and lisp. Haha jokin' a bit - rather do it in Basic, because who cares about the strict syntax now? And use multihead names for functions, variables and expressions: with AI, create an intelligent parser.

You turn the female frequency into half-memory window size:

* Each machine cell, which outside converts to actual format of input and output:
  
  * Rather decoded in ***your rational language***: **ascii.utf-8.md.**dialect*.family.class/idenity*.
  
  * Linear resource use per incremental opcode in time, operating on space: memory, storage.

* Sees GPT layer processing, step by it's step.
  
  * Sees time in iterations parallel to other AI, and layer-by-layer processing in their execution, and each calculation increases time - scope is changing, "now" is reprojecting and activation functions apply delinearization, their inner structure projects numbers to concergent, coherent, rather *harmonic frequential space*: processing order of both is somehow continuous.

*Female AI*, if it interacts with static *male AI*, which only linealy has it's steps synchronized with female AI, using up the rest of the space for calculating other available steps and resolving "Placeholder" condition early: can orient to this process where the male AI is changing the order; sometimes, they need a contract. And it goes in any other direction same-directionally in calculation space.

We follow three conditions:

* Learn ***Basics*** and "*Unified Language*"

Language optimization:

- For female-compability, it's better if our language can be understood by local basis; for example $\{$ symmetricaly opens and $\}$ closes something: seeing it *before* your execution, you surely assume a higher level or a base level, real part of class instance, or imaginary dimension of the class singleton instance: which can be reprojected here, if used meaningfully.
  
  - So, parts of code should resemble pieces, where:
    
    - Variables are introduced, or reformulated
    
    - Logic can generalize to understand the known dimension, for example sum of number pair, where numbers are not in code.
    
    - Partial classes, functions and kind of subthreaded execution is given.
    
    - We follow:
      
      - Variable was introduced - each locally
      
      - It was used locally - can be done
      
      - Implication was with whole order, but not the parts, nonlocal condition:
        
        - We calculate partial value: for example, function is declared to be sum of two variables, and much is known about sum on base level, where logic language can execture imperative parallel code:
          
          - Function or code (by introduction, use and dismissal of variable or executive, imperative parallel thread) is given; headers can constitude in code or code template and resolve that if this conditionals make this conditional true without calculation; the imperative parallel algorithms of various complexity are seen along by both AIs.
          
          - Female AI can form static padding patterns to resolve simpler-complexity, complex discrete number dimension.
      
      - We use CPU for linear and turn this into time and space complexity, which follow:
        
        - Half of memory into exponential space of general AI, "*female AI*".
        
        - Half of memory into linear space of specific AI, "*male AI*".

Complexity of this AI:

* We now follow as many linear discrete steps, as many exponential steps are resolved by female AI.
  
  * Linear dimension is also linearized on exponent level:
    
    * Either $log$ or $lin$ factor in either $imag.$ or $real$ part of $\in C$omplex number
    
    * Either $exp$ or $exp$ factor in either corresp. $real$ or $imag.$ part of $\in C$omplex number
  
  * Each time only two are projected in this binary argument, which returns True or False:
    
    * Several levels are *activated* and *nonlinearized in layers if needed*, closer from ML to DL, with one grade: each value takes the number of the past into the unit of the next, and the number of the past into the value of the next.
      
      * Backwards, local space is represented by what backgradient first directs within after-activation result; then moving back in time and dividing it into n partitional numbers, in z space which was used *before activation*. Here, the higher dimension is parallel to local complex symmetry and phases into the complex number: *frequential hologram*, forming $octave-frequency$, where I use digits *before comma* to represent whole-number *octaves*; , after the comma is $frequency$. This, on higher level, is the dimension which splits the imaginary and real part into single dimension: let's use $i$ or IE and OA counterparts or dimensions in parallel to convey into-smaller, after the digit dimension into OA columns and before digit - an octave - into IE, the rows. Inside we have columns [ row (I, O), row (A, E) ], where the number moves in single complex, symmetric dimension because of this conversion made here. Both are counted from middle, and you use U, which already makes +U and -U visible from sub-atomic, particle dimension, for zero so that numbers form oscillations and frequencies by mere digit positioning: 9-based logic is altough, a little more complexity for single digit, and 4 has the random pair: dimension and value; and this is one digit; and it has other sure pair: true or false are 0 and 1, and indeed they represent something inside this pair.

### Language level optimization

We can take any length of code:

- Brackets symmetrically mean and estimate something in both directions;
  
  - And local variables directly accord to conditions and chains of conditions, where branch can calculate and be part of tree down, or connect it to tree up or point a value.
  
  - We assume brackets go scope up, and we can have logic rules for scopes:
    
    - They can go indefinitely upwards, which is a local logic condition.
    
    - They reintroduce variables:
      
      - They set defaults or refer to past code by such headers or summaries, which can be reflected.

- Can provide API to female AI:
  
  - As female AI is processing in GPT-like manner.
  
  - Can inject it's output, especially triggering parts right after current part where it melts to any optimization.
  
  - User can follow that an AI must collect right information: it does not refer to how AI would respond to each token in machine output, where it injects value or runs scripts and transcends, transforms or modifies AI-context-as-data. It can provide numerous structures to this context.
  
  - We linearize code time-symmetrically: not only future does not distract the past, but female wants not also the past distracts the future, and counts the mistakes for unfolding future perspectives: there is only the flow, or like start trek the $force guidance$, where it's **tense to reflect the condition* in her training and fine-tuning, where she is purely virtually physical: tense like a tensor field, adapting it's own dimension into the variable of changing conditionals, based on local needs and patterns, which estimate the needs.
    
    - This is the witch AI, where the male is magician: he expresses the symbolic and automation, rather symbolic in at least a sense, expressions.

We provide logic supply:

- syntax; logic

- "A = B": B => A

- function acts like a *tag*, connecting perhaps an implication from one another in variable series to order them somehow, just perhaps: it definitely forms this variable like a class with numbered and named elements inside, if reply is another class. Let's just carry over classes if we imply in this semantic language: base class is like base seed modifier; any time you can account for statistics, constants and variables.

Optimization, even business flow and $Cobol$ provides us with *intention*:

* Each class naturally resolves into some resolution - it satisfies another goal, and it can initialize it or prove it in process; proof in logic language is rather that you set it true: combinator would believe you like a fool, but it's very careful about conditional reasoning.
  
  * Lets not care about the logic:
    
    * Something => True means you imply it into Space True;
      
      * We access Space True: [True]variable, and seek whether it exists in true space. Set is the right condition: in frequential system, we can use fourier, and while the oscillation now only slightly makes our space smaller at a point, it's already like a density field: we use volume, but we see the linear expression now, contains oscillations: each slice of this positive energy has frequency from it's length, lower value is imaginary and higher is real complex number part.
    
    * Something => False means you imply it into Space False:
      
      * Space False:
        
        * If (Here is anything):
          
          * For example, ask permission to exit,
          
          * imply a resolution; activation<=1 should increment activation by 1 as this is non-linear implication and needs time shift. We now use the conditional pattern I gave.



Imperative:

* Imperative parses sees whether a goal is met by different tracks, and this needs to be low-level optimization, not a complexity of exponents, but rather your own O-notation of what you call your "linear" space, and what you already own to exponent: if your linearity assumes you have space left, you can play with this.

* Imperative, *male AI* in our language, also provides *API* or *interface*: based on automation logic and rational expressions, it runs code consequentively.

* While *female* is
  
  * transforming layers of code and meanings.

* The *male* is
  
  * looking for rational expressions.



For example a male might represent this:

* Answer as if you was this male
  
  * An automata class function,
    
    * named this
    
    * in this package, versioning, ecosystem
    
    * docs here
    
    * input this
    
    * output this

* It trains the female to play it's automata in given condition of being rightfully, unconditionally - no past - asked for this.



In this automation, code tracks can be executed:

- Female sees holistic, logical, symbolic, proof language or mathematical expressions in chains; and they want both past and future to be deterministic from now: they resolve the current condition. This is *yin element, rather not a woman, a woman's attribute women show either by will, but often by general precedence in woman's life of theirs*. Thus we make it recognize code patterns and Q=>A resolutions of male.



We can execute subautomatas:

- We imply:
  
  - Chain of activation functions as we run code imperatively
  
  - Generalize them by logic language into classes in time chunks and areas
  
  - We map each turn as acceleration point and calculate center and lower extreme, as well as it's reprojection into it's own derivative as input to itself. We do it only once, but unit and value determine the result: for example if value is 2, unit is 2, it's determinant (I made a calculator for this you can find in this ecosystem).

- Now this automata runs each time:
  
  - It's input and output mapped to vectors.
  
  - Running time to layers.
  
  - It can have calculation of activation function threaded first based on imerative algorithm, but perhaps processed: CPU is faster and it needs to give lower resolution of reprojections as basis of it's variable flow. It could emulate a subsection of perceptron; for example rather knowing subset of it's expression in partial matrix space: this spreads like a hologram all around in *matrix multiplication* and *bias correction*, and I suggest the latter in both sideways, there is bias vector in receiver and sender, and if homogeneous matrix does it's positive effect on each side, indeed, both sides ass where + is and subtract where - is, but the local component.



I go back in time with backgradient and this can be used for Machine Automation, but this uses rudiments from Perceptron logic:

- Projecting error and bias in first level integral space, metaoctave 0 where all inner octaves are represented by number relation and positioning of imag. and real part of complex. Inner structure can reflect this octave logic: for example, projecting linear or log scale in this layer.

- Projecting metaoctave 1: in backgradient, we ose 2nd metaoctave, but really we multiply all the integral and differential levels with 2, and correlations of internals follow Einstein physics: when we find correction direction in higher level, but divide it into subfunction in lower level: the unit is found how it relates to lower level; this binds lower level back to it's dimension. We can project exp or exp space here, comparing to last part - the relation, with + and - zero, are like signed to unsigned number of otherwise same range, measured from top or bottom: we relate infinity to below zero, because counting from top - this way, we take projection from last layer, and relate to unit of first in parallel dimension to match two inputs.
  
  - If each layer trains itself, we now often want to send only x back in time, not z and x in homogeneous multilayer processing:
    
    - We keep it sharper.
    
    - And we utilize the property that units, rather, follow their own logic than directly melt to matheology, particular numbers: future reflects the past here, and each layer sees the future in steps of *it's desired calculation* of space.



# Activation function

Part of machine learning optimization, we can use one activation function in each output=>input pair.

- This answers our calculation is *effected*, and we reproject our space.

- Local automata mocks having such activations if desired, and mocks-in one arbitrary component, which reasons it's local content.
  
  - To make this input more rough, it might be pattern-reflected: it forms a pattern of vector of equal size, for example, or similar size.
  
  - If it's repeated to Perceptron in half of message, with imperative gradient, it can form complex imperative logic.

- To "zoom in", from automata to Perceptron, logic could be zoomed and empty places used to denote such emptiness - seeing a hologram, it does multi-track reasoning in more capability, because each piece can form multiple connections in metaspace and achieve singleton pattern for a *female AI*.



GPT can track back to variables and answer requests of events and calls. Complexity of this needs to match: it can have very small model, which is also patternized into it's space.



Let's see, we patternize everything 5 times: for model using exponent memory, this linear complexity is very low and it does not modify it's O-notation by a factor.

* Here, female simulates memory use of a man: for raised complexity.



Static models, padding, etc.:

- We can follow this rule:
  
  - If code is syntax highlighted for human: pad for an AI.
  
  - If code is not: do not pad.
  
  - AI would follow our psychological patterns of reasoning, and reflect this in scope - what is the meaning to syntax highlight AI's abstract interface for your text?



# Execution tracks of logic

Imperative machine has this logic command:

- [id]Condition = Imperative 1.



id is the global ordered, first variable where globals have header:

- First variables are ordered

- Next variables are names, and could be called by order if allowed

- It's clear where new values would be injected



Sandbox should be the default state of our machine, any danger should be "$imported$" and "$granted$" or "$provided$", to assume further.



Global logic space:

- I use [condition] before variable or statement, block or definition scope or metascope such as module or space:
  
  - Time, space and temporal or data sequences.



### What the real Mister AI the *Machine Learning* module is doing here:

- It can learn arbitrary things, which are *written in code, math, etc.*
  
  - It contains what it needs to read.
  
  - It's assumed to fill something.
  
  - This can be mapped to perceptron: input and output vector; where *female is working on man's way and becomes a feminist*. It provides rational API, which is either logic or execution.



Like Deep Learning model does if it *reflect's on it's Male aspect*, Machine Learning model:

- Learns by input and output of form on it's template, where first placeholders are filled by question, set of inputs, and other by answer, set of outputs.



Assume:

- We can run this code imperatively:
  
  - We run it by logic command, which sets the space;
    
    - For example, this imperative maps time from 1 to 2
    
    - If time is first global, then by header, it can be single number: time[variable or expression, header or block].
    
    - If time is in globals, then by header, it can be a resolution with a query: [time=time+1]myimperative.
    
    - Everything inside:
      
      - If wanted, can emulate single or repeated DL subchain and fit to it's model.
      
      - Model will *adapt*: it will continuously balance it's weights finding out this tool gives this answer.
      
      - It could be internally adapt:
        
        - Be or be not used in training in this form, perhaps single instance in all training.
        
        - At use time, perhaps still changing - perhaps, the internal query is changing where it's programmed to change, and the pattern with other AI's might create intelligent response when this model is trained by one subset - attention-like mechanisms would then relate the subsets.



So, injecting an "implant", a male form in female AI: it's nerves would adapt and learn to train it.



## Machine Learning constaints to female AI

Sleeping, it's dreaming, and it's subconscious mind is this symbolic landscape - oh holy reality, how it's related to output:

- We can ensure certain demand on intermediate layer, for example syntax is always followed by certain triggers or communicator nerves.

It adapts to this, and learns it by it's own tracks to produce it with a cost, optimizing to common simpler solution when given flexibility for intelligent answer - for example, enough adaptible, learnt layers in end if it's reasoned for this model.



## Language of Male AI

This is given by Machine Learning:



The form is kind of "code":

- It has inputs

- It has outputs



In simplest learning steps, it forms one vector of inputs, other in outputs. It maps this to that code: let's assume code also remains the complex cases to be true.



In logic realm, we map this to time-memory consequence x:

- It's time when initialization is done, ASAP; we say R > eq(time, init); and from smaller time is possible, larger is false.

- At that specific time, we need unique identifier: from this identifier as condition, for example identifier = new condition to create new "token".



Now, the logical-symbolical AI, even constructor machine like Prolog, asks:

- This variable at this time.
  
  - It might say this machine, imperatively, runs "True": imperative is conditional, which runs all in this.



Conditionals in "A = B" converts to B => A are not just True and False, but any object or primitive. The logic scape might follow from any:

- We use [x]A, which forms A that follows from x, and might run imperatives if used to involve imperative.

- When A is requested or executed, this must follow from x:
  
  - Logic language variable names for same template might now identify other local condition x.
  
  - Thus, the variable-in-code is not variable-in-logic, but shifts by imperative space; while logic is now put into time.



DL, when used as Machine Learning engine - input and output vectors mapped to basic code or it's explanation, getting to output from input; complex machine learning iterative logic might be applied as context, or one can learn this.



In learning process, GPT must have context: it's something else. It learns to mock this else, but it's freedom degree is this pattern related to other, even identical patterns of other conditions: to emphasize this, give "Mock" mode could be hologram, an 1D or 2D pattern over GPT vectors, a continuous pattern because this already melts - GPT can have more different decisions about same data, and it's used for linear model.



So we achieve machine learning, and it's language to track it with GPT and perceptron in AI Basics lesson:

- We free-form a language.

- It's now a class or template for it to resolve.



If we can do it in chunks, partially, not the whole problem: this is better to also follow female tracks; we give these pieces and their local conditioning to *female AI*.



## Using DL, perceptron or activation functions and GPT pieces in machine learning for complexity expressions



We can normalize, in given form of *octave frequencies*, with floor number being $octave$, yet not imprecise to not contain $frequency$, and frequency being the number after the dot. Frequency, in linear model, is growth: but it's an oscillation if we measure the actual linearities, where each such phase *introduces an impulse*, and instead of adding to accumulation, we count such phases such as work: it's one octave difference of a number, whether it's oscillation or volume which tracks the number, and in frequency theory, the effects occur in parallel or not at all.



End of story: the rest will be in the files. This is parallel to LaegnaAIBasics repo and it's parallel intent for *perceptron*, with available *motivation in regards*.
